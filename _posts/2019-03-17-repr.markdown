---
layout: post
title : "RePr 复现"
author: "siahuat0727"
catalog: true
tags:
    - RePr 复现
    - RePr implementation
---

## 前言

尝试复现 [RePr: Improved Training of Convolutional Filters](https://arxiv.org/pdf/1811.07275.pdf) 中，做个记录。
完整 code 见 [GitHub: RePr](https://github.com/siahuat0727/RePr)

## 介绍

详细的看论文啊，这里简单带过。

目标
+ 调整训练流程来避免 filters 间的 overlap。

效果
+ 在很多实验都能提升准确率。

方法

+ ![](https://i.imgur.com/p3P0OkF.png)
  白色区：S1 stage
  灰色区：S2 stage

+ S1 stage 结束时以作者提出的 inter-filter orthogonality（将 filters 各 flatten 成一维向量，计算每个 filter 与其他 filters 的向量内积之和）为评分标准，将正交性较差的进行剪枝。
+ S2 stage 过程只以剩下的 filters 进行训练，最后再将被剪枝的 filters 重新初始化训练。


## 基本做法

目前以 masking 的方式来实现。

**S1 stage**
+ S1 stage 结束时以 inter-filter orthogonality 选出不好的 filters 将 weights 设为 0。

**S2 stage**

+ 每次更新参数时把 pruned filters 的 gradient 设 0，确保过程中被 prune 的这些 filter 都还是全 0。
+ 结束时对 pruned filters 进行 reinitialize，并把这些 filters 下一层对应的 weights (channels) 也设 0（这部分还不确定）。


### Reinitialize 阶段论文里不清楚的细节 + Reddit 私讯作者的补充

+ Null space vectors 数量不够用来初始化？
    + 对 $C^3(32)$ 的模型来说，第一层有 32 个 filters，每个 filter 的参数量（若 kernel size 为 3x3）是3x3x3，这样一来 flatten 之后的矩阵是32x27，null space 是空集，没有正交的 filter。
    + 作者：
        >You can relax the requirement that the new filter has to be orthogonal to all the filters. There is no proper heuristics, but you can use orthogonal ranking to find the most overlapping ones and only use those to compute the re-initialization
    + 目前做法：以 pytorch 预设初始 conv2d weights 的方式初始化。[source](https://github.com/pytorch/pytorch/blob/08891b0a4e08e2c642deac2042a02238a4d34c67/torch/nn/modules/conv.py#L40-L47)
+ Null space vertor 与 filter 的对应关系？
    + QR decomposition （或其他方法）找到 null space 之后怎么利用这些正交的向量来对 filter 进行 reinitialize ？（如何 scaling 之类的）
    + 作者：
        >Yes, **random vectors** in the null space. Scale of this vector becomes important when you are dropping large percentage of filters. I tried various things including projecting the norm. However, I was running short on time thus I did a quick experimental thing that is **clip the weights** to be some upper-bound (if x > LIM then x = LIM).
    + 目前做法：不做 scaling ，直接把 QR decomposition 找到的任意正交向量经过 clip 后当成 filter。（这些 vectors 的 weights 的 mean, variance 都不大）

## 实验

数据集：CIFAR-10

模型：vanilla cnn $C^3(32)$
![](https://i.imgur.com/DEZUDZA.png)

### 论文结果

![](https://i.imgur.com/fzobyey.png)

Std  | RePr
---- | ----
72.1 | 76.4


### 目前结果

实验设定：
+ general：100 epochs
+ RePr 超参：S1 = 20， S2 = 10，prune_ratio = 0.3

Std   | RePr
----  | ----
77.84 | 74.05

**加上 tensorboard 视觉化 Acc：**（红色是 standard， 蓝色是 RePr）

![](https://i.imgur.com/scW5APy.png)

**与论文结果对比：**
![](https://i.imgur.com/H01zKJy.png)

**共同点：**
+ 跟论文一样会对后面 layer 的 pruning 比较多（S2 stage 中）：
	```
	filter sparsity of layer module.conv1.weight is 0.03125
	filter sparsity of layer module.conv2.weight is 0.125
	filter sparsity of layer module.conv3.weight is 0.75
	```

**不同点：**
+ training acc 曲线比较接近，但 reinitialize 之后没特别快的回升。

+ standard test acc 特别高。

+ 论文曲线 train acc 最终在 90% 左右，test acc 在 70% 左右，有较严重的 overfitting。但我们的 train acc 甚至比 test acc 略低，这是 pytorch 对 image 的预处理 random crop 造成的。若不做 random crop，则 training acc 会直接接近 100，testing acc 60 左右。（详见下面的实验结果）


### 其他尝试

#### 对应 channels 不重新初始化

一开始在 reinitialize 时没把下一层对应的 channels 设 0（但不确定这么做对不对），可以看到在 reinitialize 之后会有明显的掉点。

![](https://i.imgur.com/2qYoUMF.png)

#### 不使用 random crop，刻意让其 overfitting

在 prune 之后 test acc 反而一直掉：

![](https://i.imgur.com/ywUvvko.png)

#### 改测 CIFAR-100 数据集

Acc 曲线差不多，即目前的实现结果 RePr 较差：

![](https://i.imgur.com/sIJALFA.png)

#### 反其道而行

尝试对 inter-filter orthogonality 值较低（正交性较佳）的进行剪枝（红色曲线，浅蓝为对照组）：


![](https://i.imgur.com/pDAsssq.png)

浅蓝确实表现良好。



## 思考

目前 RePr 效果不佳可能原因：
+ RePr 流程主要提升泛化性，可能目前跑的实验比较不适用
  > **Abstract**
  > ...
  > We show that by tem-porarily pruning and then restoring a subset of the model’sfilters, and repeating this process cyclically, overlap in thelearned features is reduced,  producing **improved generalization**.
  > ...
+ 实验设定有些不同（standard 跑起来并不像论文那样 train acc 90%, test acc 70%）
+ 还有一些不清楚的细节
+ code 有 bug


## 结语

各方面新手，非常欢迎路过的有缘人给点建议啦！
